---
layout:			post
title:			"CS 439H: Operating Systems"
subtitle:		"Abstraction, resource management, and frustration"
date:			2015-12-12 04:20:00
author:			"Ashwin Madavan"
header-img:		"img/os-bg.jpg"
---

What is an operating system? Operating systems are such sprawling pieces of software that most programmers interact with them without even realizing it. Operating systems serve two purposes: managing system resources and abstracting computer hardware. Their responsibilities include facilitating multi-tasking, managing memory and disk, and providing a simple interface to application programmers for interacting with hardware and system resources.

In this class, we built a (shitty) operating system from scratch. Beginning from just a master boot record, we designed a system capable of executing multiple c programs concurrently, complete with its own file system, system call interface, shell, and automatic reference counter. Not too bad for one semester! 

# Project 1: Cooperative Threading
It may seem that you are running multiple programs simultaneously on your computer; in reality, processors can only perform one task at a time. Therefore, operating systems have to execute the programs in a manner that seems parallel but is actually sequential. One way of implementing interleaved execution is cooperative multitasking. A cooperative multitasking system cedes the responsibility of multiplexing user programs to the user programs themselves, who must decide when they want to *yield* to each other.

Our implementation of cooperative threading utilizes a simple first-in-first-out (FIFO) scheduler. The kernel maintains a ready queue of threads awaiting execution. Every time a program yields, it is placed on the end of the queue and the thread at the head of the queue is run. This procedure, known as a context switch, must first save the state of the active thread (registers, stack pointer, etc.) before switching, to ensure that the thread can pick up where it left off next time it is run. Our implementation of context switch only saves callee-saved registers (yield is a function call) and the stack pointer, but as additional features are added to our threading package we will need to save additional information in our context switch.

{% highlight asm linenos %}
# void contextSwitch(int* oldSp, int* newSp)
	.global contextSwitch
contextSwitch:
	movl	4(%esp), %eax
	movl	8(%esp), %ecx

	# Push the old thread state onto the stack and save the old stack pointer
	pushl	%esi
	pushl	%edi
	pushl	%ebp
	pushl	%ebx
	movl	%esp, (%eax)
	
	# Retrieve the new stack pointer and pop the values off the stack
	movl	(%ecx), %esp
	popl	%ebx
	popl	%ebp
	popl	%edi
	popl	%esi
	
	ret
{% endhighlight %}

# Project 2: Preemptive Threading and Memory Heap
Cooperative multitasking is a good solution if user programs are guaranteed to work together. In reality, it is unlikely that user programs will multiplex system resources efficiently and improbable that all user programs will be cooperative. Therefore, modern operating systems also implement preemptive multitasking. A preemptively multitasking system interrupts user programs at regular intervals to ensure that all processes are given a chance to run. 

To implement preemptive threading, we make use of interrupts, the interrupt descriptor table (IDT), and the programmable interval timer (PIT). Interrupts are triggered by hardware or software to inform the processor of some event that requires attention. The processor stops what it is doing, saves its current state, locates the interrupt handler (a function) associated with the interrupt in the IDT, executes the handler, and then resumes what it was working on previously. The PIT is programmed to regularly trigger an interrupt whose interrupt handler forces the processor to perform a context switch.

At this point, we have a system capable of interleaving program execution. However, this system has no mechanism for preventing programs from overwriting each other's memory and for reallocating memory. A simple solution to the first problem is a naive heap consisting of a single pointer. Each time a request to allocate memory (malloc) comes in, the naive heap returns the pointer and increments it by the number of requested bytes. However, this implementation does not solve the second problem; once a memory address has been allocated, there is no way for that address to be reused for any other purpose. Therefore, a good heap must also keep track of available blocks of memory so that blocks can be reallocated once they are no longer in use.

<img align="center" style="margin: 0 auto; display: block;" src="/img/os-heap.png">

The left image in the picture above shows the proposed design of a block. Blocks contain a header and footer that store the total size of the block, pointers to the previous and next available blocks, as well as a region where data can be stored. The sign of the header and footer indicates whether or not the block is available; if positive the block is available, and if negative the block is allocated. The right image shows what occurs when malloc is called. The heap traverses the linked list of free blocks using the next pointers and locates the first block whose total size is greater than the number of requested bytes + 8 (adjusted for heap overhead). The heap then splits the block into two pieces, updates the header and footer information of each piece, and returns a pointer to the data region of the second piece.

{% highlight c linenos %}
void* malloc(size_t bytes) {
	// Empty List (return 0)
	if(head == 0) return 0;

	// Minimum Length (16 = header + footer + 2 LL pointers)
	int length = (bytes < 8) ? 16 : bytes + 8;

	// First Fit (first block big enough)
	Block *block = head;
	while(*((int*) block - 1) <= length) {
		if(block->next == 0)
			return 0;
		block = block->next;
	}

	int *header  = (int*) block - 1;
	int *footer  = (int*)((int) header + *header) - 1;

	if(*header - length < 16) {
		// Return Block
		remove(block);
		*header *= -1;
		*footer *= -1;
		return (void*) block;
	} else {
		// Split Block
		*footer  = -length;				// New Footer
		footer   = (int*)((int) footer - length);
		*footer  = *header - length;			// Old Footer
		*(footer + 1) = -length;			// New Header
		*(header + 0) = *footer;			// Old Header
		return (void*)(footer + 2);
	}
}       
{% endhighlight %}

A naive implementation of free might flip the signs of the blocks header and footer and subsequently add it to the linked list of available blocks so that it can be reallocated in the future. This implementation is poor, because it could lead to memory fragmentation. To understand why, imagine a system in which a user program first allocates enough ints to fill physical memory and then frees all of them. The linked list of available blocks would then contain one entry for each allocated integer. Even though no physical memory is in use, the heap will be unable to service requests for more than 4 bytes. because every available block in the linked list is of size 4. A better implementation of free would merge adjacent available blocks to avoid memory fragmentation.

{% highlight c linenos %}
void free(void* p) {
	// Retrieve Block
	Block *block = (Block*) p;
	int *header  = (int*) block - 1;
	*header     *= -1;
	int *footer  = (int*)((int) header + *header) - 1;
	*footer     *= -1;

	// Merge Right
	if(*(footer + 1) > 0) {
		remove((Block*)(footer + 2));
		*header += *(footer + 1);
		footer   = (int*)((int) header + *header) - 1;
		*footer  = *header; 
	}

	// Merge Left
	if(*(header - 1) > 0) {
		block 	 = (Block*)((int) block - *(header - 1));
		remove(block);
		*footer += *(header - 1);
		header   = (int*)((int) footer - *footer) + 1;
		*header  = *footer;
	}

	// Add Block
	add(block);
}
{% endhighlight %}

# Project 3: Synchronization
With preemption comes every programmers' worst nightmare - race conditions. Suppose you have two threads that are both trying to flip the value of some bit. Normally, you would expect something like the following to occur.

| thread 1 | thread 2 | bit   |
|:--------:+:--------:+:-----:|
| read     |          | 0     |
| flip	   |          | 1     |
|          | read     | 1     |
|          | flip     | 0     |

Remember, threads can be preempted at *any* time. Suppose thread 1 is preempted after it has read the value, but before it flips the bit. Then, thread 2 will read the *old* value of the bit. This sequence of events produces incorrect output.

| thread 1 | thread 2 | bit   |
|:--------:+:--------:+:-----:|
| read     |          | 0     |
|    	   | read     | 0     |
| flip     |          | 1     |
|          | flip     | 1     |

What can we do to prevent this problem from occurring? One way to deal with race conditions is to disable preemption within *critical sections*, or sections of code that can be executed by multiple threads or access shared resources. However, this solution is only acceptable for short critical sections, because it prevents other threads from running for the duration of the critical section. This simple solution is implemented as a *spin lock* in Linux. But what do we do for longer critical sections? How do we eliminate race conditions without preventing the rest of the system from making progress? The answer lies in the semaphore.

Suppose you are the cashier at a restaurant and you are helping a customer when another customer walks in. Because you can only serve one customer at a time, the customer waits in line until it is their turn to order. As customers keep coming in, they will continue to form a *queue* until you are ready to help them. Now suppose there are *two* cash registers in the restaurant, so that two customers can be served at once; customers still form a queue, but only if there are more than two of them in the restaurant. This metaphor describes the abstract behavior of a semaphore. A semaphore contains a count, a queue as well as a spin lock to prevent race conditions on semaphore operations, as well as two methods: *down* and *up*.

| method | condition        | effect     	             | analogy                                               |
|:------:+:-----------------+:---------------------------+:------------------------------------------------------|
| down   | count > 0        | count decremented          | available cashiers; so customer is immediately served |
| down   | count == 0       | thread added to queue      | no available cashiers; so customer waits in line      |
| up     | queue.isEmpty()  | count incremented          | no waiting customers; so cashier is available/idle    |
| up     | !queue.isEmpty() | thread removed from queue  | waiting customers; so next customer in line is served |

Below is a simple implementation of a semaphore. 
 
{% highlight c++ linenos %}
class Semaphore {
	uint32_t count;
	SpinLock *spin;
	SimpleQueue<Thread> *queue;

public:
    Semaphore(uint32_t count) : count(count), 
		spin(new SpinLock()), queue(new SimpleQueue<Thread>()) {}
	
    void down() {
		// Lock the semaphore
		bool was = spin -> lock();

		// If not at capacity, then decrement and unlock. Otherwise, add 
		// the thread to the blocked queue and remove it from the ready queue.
		if(count > 0) {
			count--;
			spin -> unlock(was);
		} else {
			queue -> add(Thread::current());
			threadBlock(spin, was);
		}
	}

    void up() {
		// Lock the semaphore
		bool was = spin -> lock();

		// If there are no blocked threads, then increment the counter and
		// continue execution. Otherwise, retrieve the next waiting thread
		// from the blocked queue and add it to the ready queue.
		if(count > 0) {
			count++;
		} else {
			// If the blocked threads queue is empty, then increment the
			// counter and continue execution. Otherwise, add the thread
			// to the ready queue and do not increment the counter.
			Thread *head = queue -> remove();
			if(head == nullptr) {
				count++;
			} else {
				threadReady(head);
			}
		}

		// Unlock the semaphore
		spin -> unlock(was);	
	}
};
{% endhighlight %}

Another interesting problem in concurrency is the Producer-Consumer Problem. Suppose you have two threads and a shared buffer of finite size. The first thread is attempting to *put* elements in the buffer (producer) and the second thread is attempting to *remove* elements from the buffer (consumer). How do we ensure that the producer only places elements in the buffer when it is not full, and the consumer only removes elements from the buffer when it is not empty? The solution to this problem is to use bounded buffers, which make clever use of semaphores to ensure that the producer waits until there is free space in the buffer and the consumer waits until there is data in the buffer. My implementation of bounded buffers utilizes a circular array, two semaphores, and a mutex to solve the producer-consumer problem.

{% highlight c++ linenos %}
class BoundedBuffer {
	int *data;
	int n;	
	int pindex;
	int gindex;
	
	Semaphore *sem;
	Semaphore *put;
	Semaphore *get;

public:
    BoundedBuffer(int n) : data(new int[n]), n(n), pindex(0), gindex(0) {
		sem = new Semaphore(1);
		put = new Semaphore(n);
		get = new Semaphore(0);
	}   		

    void put(int v) {
		// Down the put semaphore; block until there is space available.
		put -> down();

		// This semaphore acts as a mutex to prevent multiple threads from 
		// interacting with the buffer at once.
		sem -> down();		

		// Add the value and adjust the pointer
		data[pindex] = v;
		pindex = (pindex + 1) % n;

		// Unlock the mutex to allow other threads to use buffer
		sem -> up();

		// Up the get semaphore to indicate that data is available to be read.
		get -> up();	
	} 

    int get(void) {
		// Down the get semaphore; block until there are elements to be read.
		get -> down();

		// Lock the buffer to prevent race conditions.
		sem -> down();

		// Retrieve the next element in the buffer.
		int v = data[gindex];
		gindex = (gindex + 1) % n;
		
		// Unlock the buffer
		sem -> up();

		// Up the put semaphore to indicate that there is available space.
		put -> up();

		return v;
    }

};
{% endhighlight %}

In addition to these synchronization mechanisms, we also built events and barriers. Threads can *wait* on events, and when another thread *signals* that the event is complete, the waiting thread resumes execution. Barriers prevent threads from continuing until a set number of threads reach the barrier.

# Project 4: Reference Counting
We now have a system capable of two forms of multithreading, complete with its own heap as well as an assortment of synchronization mechanisms to eliminate pesky race conditions and facilitate parallelized computation, and its only week 3!

The next problem we have to consider is memory leaks. How do we ensure that objects are destructed when they are no longer in use and go out of scope? There are three places where a variable can "live": stack frame, heap, and global memory. Objects in global memory do not ever go out of scope (definition of global), and it is trivial to detect when objects on the stack go out of scope (whenever they are popped off). The real challenge is detecting when heap allocated variables should be destructed.

There are two competing philosophies about how to clean up heap allocated variables: tracing garbage collection (GC) and automatic reference counting (ARC). GC takes an active approach to memory management; for example, a mark-and-sweep garbage collector will look through the stack frames and global memory of all threads, marks all referenced objects in the heap, and then scans the heap and destroys all unmarked objects. In contrast, an automatic reference counter keeps track of the number of references to an object and destroys the object when there are no references to it.

The benefit of tracing garbage collection is that it is simpler to implement and does not incur any extra overhead when accessing shared pointers. The downside of tracing garbage collection is that they take a significant amount of memory to run and prevent the system from making progress while they are running. 

<img align="center" style="margin: 0 auto; display: block;" src="/img/os-strong-ptr.png">

We implemented automatic reference counting by creating a new pointer type, *Strong Ptr*. Strong pointers contain a pointers to a counter and the underlying object. When the counter reaches zero, there are no more references to the object and the object is destructed. Below is a simple implementation of strong pointers.

{% highlight c linenos %}
template <class T>
class StrongPtr {
	T* pointer;
	int* refs;

public:
    // Construct a null reference
    StrongPtr() : pointer(nullptr), refs(nullptr) {}

    // Construct a reference to the given object
    explicit StrongPtr(T* ptr) { 
		pointer = ptr;
		if(ptr == nullptr) {
			refs = nullptr;
		} else {
			refs = new int();
			*refs = 1;
		}
	}

	// Reference the same object referenced by src
    StrongPtr(const StrongPtr& src) {
		pointer = src.pointer;
		refs = src.refs;
		
		if(refs != nullptr)
			*refs += 1;
	}

    // Steal the reference from src
    StrongPtr(StrongPtr&& src) {
		pointer = src.pointer;
		refs = src.refs;

		if(refs != nullptr)
			*refs += 1;
		
		src.reset();
	}

	// Remove one reference
    ~StrongPtr() {
		reset();
	}

	// Return the referenced object
    T* operator -> () {
        return pointer;
    }

	// Check if the reference object is nullptr
	bool isNull() {
        return pointer == nullptr;
	}

	// Set the reference to null
	void reset() {
		if(pointer != nullptr) {
			// Atomically decrement the reference count
			if(getThenIncrement(refs, -1) <= 1) {
				delete pointer;
				delete refs;
			}

			pointer = nullptr;
			refs = nullptr;
		}
	}

	// Assignment, copy the src
	StrongPtr<T>& operator = (const StrongPtr& src) {
		reset();

		if(src.pointer != nullptr) {
			pointer = src.pointer;
			refs = src.refs;
			*refs += 1;
		}

		return *this;
    }

	// Assignment, steal the src
	StrongPtr<T>& operator = (StrongPtr&& src) {
		reset();

		if(src.pointer != nullptr) {
			pointer = src.pointer;
			refs = src.refs;
			*refs += 1;
			src.reset();
		}       

		return *this;
	}

	// Do this and other reference the same object
	bool operator ==(const StrongPtr<T>& other) {
        return pointer == other.pointer;
	}

	// Do this and other reference different objects
	bool operator !=(const StrongPtr<T>& other) {
        return pointer != other.pointer;
	}
};
{% endhighlight %}

# Project 5: I/O

# Project 6: File Systems

# Project 7: Virtual Memory

# Project 8: Processes and System Calls

# Project 9: File System Calls

# Project 10: Shell

# Project 11: Pipes, User Threads, and Exit Statuses

# Project 12: Kernel Durability

# Project 13: Completely Fair Scheduler (CFS)

Cover photograph by [9to5mac.com](http://9to5mac.com/2014/08/18/here-are-all-of-os-x-yosemites-beautiful-new-wallpapers/#jp-carousel-336329).
