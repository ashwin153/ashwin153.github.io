---
layout:			post
title:			"CS 439H: Operating Systems"
subtitle:		"WIP: Abstraction, resource management, and frustration"
date:			2015-12-12 04:20:00
author:			"Ashwin Madavan"
header-img:		"img/os-bg.jpg"
---

What is an operating system? Operating systems are such sprawling pieces of software that most programmers interact with them without even realizing it. Operating systems serve two purposes: managing system resources and abstracting computer hardware. Their responsibilities include facilitating multi-tasking, managing memory and disk, and providing a simple interface to application programmers for interacting with hardware and system resources.

In this class, we built a (shitty) operating system from scratch. Beginning from just a master boot record, we designed a system capable of executing multiple c programs concurrently, complete with its own file system, system call interface, shell, and automatic reference counter. Not too bad for one semester! 

# Project 1: Cooperative Threading
It may seem that you are running multiple programs simultaneously on your computer; in reality, processors can only perform one task at a time. Therefore, operating systems have to execute programs in a manner that seems parallel but is actually sequential. One way of implementing interleaved execution is cooperative multitasking. A cooperative multitasking system cedes the responsibility of multiplexing user programs to the user programs themselves, who must decide when they want to *yield* to each other.

Our implementation of cooperative threading utilizes a simple first-in-first-out (FIFO) scheduler. The kernel maintains a ready queue of threads awaiting execution. Every time a program yields, it is placed on the end of the queue and the thread at the head of the queue is run. This procedure, known as a context switch, must first save the state of the active thread (registers, stack pointer, etc.) before switching, to ensure that the thread can pick up where it left off next time it is run. Our implementation of context switch only saves callee-saved registers (yield is a function call) and the stack pointer, but as additional features are added to our threading package we will need to save additional information in our context switch.

{% highlight asm linenos %}
# void contextSwitch(int* oldSp, int* newSp)
	.global contextSwitch
contextSwitch:
	movl	4(%esp), %eax
	movl	8(%esp), %ecx

	# Push the old thread state onto the stack and save the old stack pointer
	pushl	%esi
	pushl	%edi
	pushl	%ebp
	pushl	%ebx
	movl	%esp, (%eax)
	
	# Retrieve the new stack pointer and pop the values off the stack
	movl	(%ecx), %esp
	popl	%ebx
	popl	%ebp
	popl	%edi
	popl	%esi
	
	ret
{% endhighlight %}

# Project 2: Preemptive Threading and Memory Heap
Cooperative multitasking is a good solution if user programs are guaranteed to work together. In reality, it is unlikely that user programs will multiplex system resources efficiently and improbable that all user programs will be cooperative. Therefore, modern operating systems also implement preemptive multitasking. A preemptively multitasking system interrupts user programs at regular intervals, to ensure that all processes are given a chance to run. 

To implement preemptive threading, we make use of interrupts, the interrupt descriptor table (IDT), and the programmable interval timer (PIT). Interrupts are triggered by hardware or software to inform the processor of some event that requires attention. The processor stops what it is doing, saves its current state, locates the interrupt handler (a function) associated with the interrupt in the IDT, executes the handler, and then resumes what it was working on previously. The PIT is programmed to regularly trigger an interrupt whose interrupt handler forces the processor to perform a context switch.

At this point, we have a system capable of interleaving program execution. However, this system has no mechanism for preventing programs from overwriting each other's memory and for reallocating memory. A simple solution to the first problem is a naive heap consisting of a single pointer. Each time a request to allocate memory (malloc) comes in, the naive heap returns the pointer and increments it by the number of requested bytes. However, this implementation does not solve the second problem; once a memory address has been allocated, there is no way for that address to be reused for any other purpose. Therefore, a good heap must also keep track of available blocks of memory so that blocks can be reallocated once they are no longer in use.

<img align="center" style="margin: 0 auto; display: block;" src="/img/os-heap.png">

The left image in the picture above shows the proposed design of a block. Blocks contain a header and footer that store the total size of the block, pointers to the previous and next available blocks, as well as a region where data can be stored. The sign of the header and footer indicates whether or not the block is available; if positive the block is available, and if negative the block is allocated. The right image shows what occurs when malloc is called. The heap traverses the linked list of free blocks using the next pointers and locates the first block whose total size is greater than the number of requested bytes + 8 (adjusted for heap overhead). The heap then splits the block into two pieces, updates the header and footer information of each piece, and returns a pointer to the data region of the second piece.

{% highlight c linenos %}
void* malloc(size_t bytes) {
	// Empty List (return 0)
	if(head == 0) return 0;

	// Minimum Length (16 = header + footer + 2 LL pointers)
	int length = (bytes < 8) ? 16 : bytes + 8;

	// First Fit (first block big enough)
	Block *block = head;
	while(*((int*) block - 1) <= length) {
		if(block->next == 0)
			return 0;
		block = block->next;
	}

	int *header  = (int*) block - 1;
	int *footer  = (int*)((int) header + *header) - 1;

	if(*header - length < 16) {
		// Return Block
		remove(block);
		*header *= -1;
		*footer *= -1;
		return (void*) block;
	} else {
		// Split Block
		*footer  = -length;				// New Footer
		footer   = (int*)((int) footer - length);
		*footer  = *header - length;			// Old Footer
		*(footer + 1) = -length;			// New Header
		*(header + 0) = *footer;			// Old Header
		return (void*)(footer + 2);
	}
}       
{% endhighlight %}

A naive implementation of free might flip the signs of the blocks header and footer and subsequently add it to the linked list of available blocks so that it can be reallocated in the future. This implementation is poor, because it could lead to memory fragmentation. To understand why, imagine a system in which a user program first allocates enough integers (4 bytes) to fill physical memory and then frees all of them. The linked list of available blocks would then contain one entry for each allocated integer. Even though no physical memory is in use, the heap will be unable to service requests for more than 4 bytes, because every block in the linked list is of size 4. A better implementation of free would merge adjacent available blocks to avoid memory fragmentation.

{% highlight c linenos %}
void free(void* p) {
	// Retrieve Block
	Block *block = (Block*) p;
	int *header  = (int*) block - 1;
	*header     *= -1;
	int *footer  = (int*)((int) header + *header) - 1;
	*footer     *= -1;

	// Merge Right
	if(*(footer + 1) > 0) {
		remove((Block*)(footer + 2));
		*header += *(footer + 1);
		footer   = (int*)((int) header + *header) - 1;
		*footer  = *header; 
	}

	// Merge Left
	if(*(header - 1) > 0) {
		block 	 = (Block*)((int) block - *(header - 1));
		remove(block);
		*footer += *(header - 1);
		header   = (int*)((int) footer - *footer) + 1;
		*header  = *footer;
	}

	// Add Block
	add(block);
}
{% endhighlight %}

# Project 3: Synchronization
With preemption comes every programmers' worst nightmare - race conditions. Suppose you have two threads that are both trying to flip the value of some bit. Normally, you would expect something like the following to occur.

| thread 1 | thread 2 | bit   |
|:--------:+:--------:+:-----:|
| read     |          | 0     |
| flip	   |          | 1     |
|          | read     | 1     |
|          | flip     | 0     |

Remember, threads can be preempted at *any* time. Suppose thread 1 is preempted after it has read the value, but before it flips the bit. Then, thread 2 will read the *old* value of the bit. This sequence of events produces incorrect output.

| thread 1 | thread 2 | bit   |
|:--------:+:--------:+:-----:|
| read     |          | 0     |
|    	   | read     | 0     |
| flip     |          | 1     |
|          | flip     | 1     |

What can we do to prevent this problem from occurring? One way to deal with race conditions is to disable preemption within *critical sections*, or sections of code that can be executed by multiple threads or access shared resources. However, this solution is only acceptable for short critical sections, because it prevents other threads from running for the duration of the critical section. This simple solution is implemented as a *spin lock* in Linux. But what do we do for longer critical sections? How do we eliminate race conditions without preventing the rest of the system from making progress? The answer lies in the semaphore.

Suppose you are the cashier at a restaurant and you are helping a customer when another customer walks in. Because you can only serve one customer at a time, the customer waits in line until it is their turn to order. As customers keep coming in, they will continue to form a *queue* until you are ready to help them. Now suppose there are *two* cash registers in the restaurant, so that two customers can be served at once; customers still form a queue, but only if there are more than two of them in the restaurant. This metaphor describes the abstract behavior of a semaphore. A semaphore contains a count, a queue, and a spin lock to prevent race conditions on semaphore operations, as well as two methods: *down* and *up*.

| method | condition        | effect     	             | analogy                                               |
|:------:+:-----------------+:---------------------------+:------------------------------------------------------|
| down   | count > 0        | count decremented          | available cashiers; so customer is immediately served |
| down   | count == 0       | thread added to queue      | no available cashiers; so customer waits in line      |
| up     | queue.isEmpty()  | count incremented          | no waiting customers; so cashier is available/idle    |
| up     | !queue.isEmpty() | thread removed from queue  | waiting customers; so next customer in line is served |

Below is a simple implementation of a semaphore. 
 
{% highlight c++ linenos %}
class Semaphore {
	uint32_t count;
	SpinLock *spin;
	SimpleQueue<Thread> *queue;

public:
    Semaphore(uint32_t count) : count(count), 
		spin(new SpinLock()), queue(new SimpleQueue<Thread>()) {}
	
    void down() {
		// Lock the semaphore
		bool was = spin -> lock();

		// If not at capacity, then decrement and unlock. Otherwise, add 
		// the thread to the blocked queue and remove it from the ready queue.
		if(count > 0) {
			count--;
			spin -> unlock(was);
		} else {
			queue -> add(Thread::current());
			threadBlock(spin, was);
		}
	}

    void up() {
		// Lock the semaphore
		bool was = spin -> lock();

		// If there are no blocked threads, then increment the counter and
		// continue execution. Otherwise, retrieve the next waiting thread
		// from the blocked queue and add it to the ready queue.
		if(count > 0) {
			count++;
		} else {
			// If the blocked threads queue is empty, then increment the
			// counter and continue execution. Otherwise, add the thread
			// to the ready queue and do not increment the counter.
			Thread *head = queue -> remove();
			if(head == nullptr) {
				count++;
			} else {
				threadReady(head);
			}
		}

		// Unlock the semaphore
		spin -> unlock(was);	
	}
};
{% endhighlight %}

Another interesting problem in concurrency is the Producer-Consumer Problem. Suppose you have two threads and a shared buffer of finite size. The first thread is attempting to *put* elements in the buffer (producer) and the second thread is attempting to *remove* elements from the buffer (consumer). How do we ensure that the producer only places elements in the buffer when it is not full, and the consumer only removes elements from the buffer when it is not empty? The solution to this problem is to use bounded buffers, which make clever use of semaphores to ensure that the producer waits until there is free space in the buffer and the consumer waits until there is data in the buffer. My implementation of bounded buffers utilizes a circular array, two semaphores, and a mutex to solve the producer-consumer problem.

{% highlight c++ linenos %}
class BoundedBuffer {
	int *data;
	int n;	
	int pindex;
	int gindex;
	
	Semaphore *sem;
	Semaphore *put;
	Semaphore *get;

public:
    BoundedBuffer(int n) : data(new int[n]), n(n), pindex(0), gindex(0) {
		sem = new Semaphore(1);
		put = new Semaphore(n);
		get = new Semaphore(0);
	}   		

    void put(int v) {
		// Down the put semaphore; block until there is space available.
		put -> down();

		// This semaphore acts as a mutex to prevent multiple threads from 
		// interacting with the buffer at once.
		sem -> down();		

		// Add the value and adjust the pointer
		data[pindex] = v;
		pindex = (pindex + 1) % n;

		// Unlock the mutex to allow other threads to use buffer
		sem -> up();

		// Up the get semaphore to indicate that data is available to be read.
		get -> up();	
	} 

    int get(void) {
		// Down the get semaphore; block until there are elements to be read.
		get -> down();

		// Lock the buffer to prevent race conditions.
		sem -> down();

		// Retrieve the next element in the buffer.
		int v = data[gindex];
		gindex = (gindex + 1) % n;
		
		// Unlock the buffer
		sem -> up();

		// Up the put semaphore to indicate that there is available space.
		put -> up();

		return v;
    }

};
{% endhighlight %}

In addition to these synchronization mechanisms, we also built events and barriers. Threads can *wait* on events, and when another thread *signals* that the event is complete, the waiting threads resumes execution. Barriers force threads to wait until a set number of threads reach the barrier. These synchronization techniques will become useful later on when we start developing more complicated abstractions.

# Project 4: Reference Counting
We now have a system capable of two forms of multithreading, complete with its own heap as well as an assortment of synchronization mechanisms to eliminate pesky race conditions and facilitate parallelized computation.

The next problem we have to consider is memory leaks. How do we ensure that objects are destructed when they go out of scope? There are three places where an object can live: on the stack, in the heap, or in global memory. However, it is trivial to detect when stack-allocated objects go out of scope (when they are popped off the stack) and global objects *never* go out of scope (definition of global). Therefore, the real challenge is detecting when heap-allocated objects should be destructed.

We discussed two solutions for managing heap-allocated objects: tracing garbage collection and automatic reference counting. Tracing garbage collection takes an active approach to memory management; for example, the mark-and-sweep garbage collector looks through the stack frames and global memory of all running threads, marks all referenced objects within the heap, and then scans through the heap and frees all unmarked objects. In contrast, an automatic reference counter cedes the responsibility of memory management to compilers who use **smart pointers** to track the number of references to a particular instance of an object. Smart pointers are objects that simulate pointers, but also provide additional features such as reference counting or address validation.

The benefit of tracing garbage collection is that it is simpler to implement and does not incur any extra overhead that comes from accessing smart pointers. The downside of tracing garbage collection is that it takes a significant amount of memory to run, prevents the system from making progress while it is running, and is non-deterministic (there is no way of knowing when a object will be destructed). 

<img align="center" style="margin: 0 auto; display: block;" src="/img/os-strong-ptr.png">

We implemented automatic reference counting by creating a new smart pointer type, *Strong Ptr*. Strong pointers contain pointers to a both a reference counter and the underlying object. Each time a reference to the object is created, the counter is incremented; each time a reference to the object is lost, the counter is decremented. Once the counter reaches zero, the ARC knows that there are no references to the object and, therefore, that it can safely call the object's destructor. Below is a simple implementation of a generic strong pointer.

{% highlight c linenos %}
template <class T>
class StrongPtr {
	T* pointer;
	int* refs;

public:
    // Construct a null reference
    StrongPtr() : pointer(nullptr), refs(nullptr) {}

    // Construct a reference to the given object
    explicit StrongPtr(T* ptr) { 
		pointer = ptr;
		if(ptr == nullptr) {
			refs = nullptr;
		} else {
			refs = new int();
			*refs = 1;
		}
	}

	// Reference the same object referenced by src
    StrongPtr(const StrongPtr& src) {
		pointer = src.pointer;
		refs = src.refs;
		
		if(refs != nullptr)
			*refs += 1;
	}

    // Steal the reference from src
    StrongPtr(StrongPtr&& src) {
		pointer = src.pointer;
		refs = src.refs;

		if(refs != nullptr)
			*refs += 1;
		
		src.reset();
	}

	// Remove one reference
    ~StrongPtr() {
		reset();
	}

	// Return the referenced object
    T* operator -> () {
        return pointer;
    }

	// Check if the reference object is nullptr
	bool isNull() {
        return pointer == nullptr;
	}

	// Set the reference to null
	void reset() {
		if(pointer != nullptr) {
			// Atomically decrement the reference count
			if(getThenIncrement(refs, -1) <= 1) {
				delete pointer;
				delete refs;
			}

			pointer = nullptr;
			refs = nullptr;
		}
	}

	// Assignment, copy the src
	StrongPtr<T>& operator = (const StrongPtr& src) {
		reset();

		if(src.pointer != nullptr) {
			pointer = src.pointer;
			refs = src.refs;
			*refs += 1;
		}

		return *this;
    }

	// Assignment, steal the src
	StrongPtr<T>& operator = (StrongPtr&& src) {
		reset();

		if(src.pointer != nullptr) {
			pointer = src.pointer;
			refs = src.refs;
			*refs += 1;
			src.reset();
		}       

		return *this;
	}

	// Do this and other reference the same object
	bool operator ==(const StrongPtr<T>& other) {
        return pointer == other.pointer;
	}

	// Do this and other reference different objects
	bool operator !=(const StrongPtr<T>& other) {
        return pointer != other.pointer;
	}
};
{% endhighlight %}

# Project 5: I/O

# Project 6: File Systems

# Project 7: Virtual Memory

# Project 8: Processes and System Calls

# Project 9: File System Calls

# Project 10: Shell

{% highlight c++ linenos %}
int exec(char* name, int nargs, char** args) { 
	// Lookup program in the file system. Make sure the file exists, and
	// that it is executable (all executable ELF files begin with a
	// special four byte sequence).
	StrongPtr<File> file = kernelInfo->rootDir->lookupFile(name);
	if(file.isNull()) return -1;
	char* buf = new char[4];
	file->read(0, buf, 4);
	if(buf[0]!=0x7f || buf[1]!='E' || buf[2]!='L' || buf[3]!='F') return -1;

	// Push (1) arguments onto the user stack, (2) push pointers to
	// those arguments, (3) push pointer to the pointers, and (4)
	// push the number of arguments onto the stack.
	uint32_t *base = (uint32_t*) 0xffffff00;
	uint32_t *ptrs = new uint32_t[esp[2]];

	for(uint32_t i = 0; i < nargs; i++) {
		char* ptr = args[i];
		if(ptr < USERLAND || String::strlen(ptr) < 0) return -1;
		uint32_t len = String::strlen(ptr)+1;
		base = (uint32_t*)((uint32_t) base - len);
		memcpy(base, ptr, len);
		ptrs[i] = (uint32_t) base;
	}   
		
	base -= sizeof(uint32_t*) * nargs;
	memcpy(base, ptrs, sizeof(uint32_t*) * nargs);
	base -= 2;
	*(base+0) = nargs;
	*(base+1) = (uint32_t) (base + 2);
	switchToUser(ELF::load(file), (uint32_t) base, 0x0);
}
{% endhighlight %}

# Project 11: Pipes, User Threads, and Exit Statuses
{% highlight c++ linenos %}
#define PIPE_SIZE 1024

class Pipe : public File {

	StrongPtr<BoundedBuffer<char>> rbuf;
	StrongPtr<BoundedBuffer<char>> wbuf;

public:

	Pipe() {
		rbuf = StrongPtr<BoundedBuffer<char>>(new BoundedBuffer<char>(PIPE_SIZE));
		wbuf = StrongPtr<BoundedBuffer<char>>();
	}

	Pipe(Pipe* pipe) {
		rbuf = StrongPtr<BoundedBuffer<char>>(new BoundedBuffer<char>(PIPE_SIZE));
		wbuf = pipe->rbuf;
		pipe->wbuf = rbuf;
	}

	inline uint32_t getType() {
		return -1;
	}

	inline uint32_t getLength() {
		return -1;
	}

	size_t read(size_t offset, void* buf, size_t nbytes) {
		return readAll(offset, buf, nbytes);
	}

	size_t readAll(size_t offset, void* buf, size_t nbytes) {
		char* buffer = (char*) buf;
		for(uint32_t i = 0; i < nbytes; i++)
			buffer[i] = rbuf->get();
		return nbytes;
	}

	size_t write(size_t offset, void* buf, size_t nbytes) {
		char* buffer = (char*) buf;
		for(uint32_t i = 0; i < nbytes; i++)
			wbuf->put(buffer[i]);
		return nbytes;
	}
};
{% endhighlight %}

# Project 12: Kernel Durability

# Project 13: Completely Fair Scheduler (CFS)

Cover photograph by [9to5mac.com](http://9to5mac.com/2014/08/18/here-are-all-of-os-x-yosemites-beautiful-new-wallpapers/#jp-carousel-336329).
